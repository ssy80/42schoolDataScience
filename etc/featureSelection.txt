1. PCA (Principal Component Analysis)

PCA transforms your features into new, uncorrelated components.

Each principal component captures the maximum variance.

You lose some interpretability (because PCs are linear combinations, not original features).

üëâ Good when:

You have many correlated features (e.g. Empowered, Stims, Recovery all move together).

You want to reduce dimensions (e.g. 30 features ‚Üí 5 PCs).

Your main goal is prediction, not interpretability.

2. Feature Selection

Instead of transforming features, we choose the most important ones.

a) Correlation-based (what we did earlier)

Keep features with correlation > threshold.

Simple, but ignores feature interactions.

b) Model-based (Embedded methods)

Use a model that gives feature importance:


Which to use?

If you want interpretability (knowing which stats matter most for Jedi vs Sith):
‚Üí Use feature selection (Random Forest, Lasso, or correlation filtering).

If you want dimensionality reduction & efficiency (many features, collinearity):
‚Üí Use PCA (but you lose original feature meaning).



Perfect üöÄ That‚Äôs a really strong approach:

Select the most important features (filter out the weak/noisy ones).

Apply PCA on the reduced feature set to remove collinearity & compress into fewer dimensions.



What this does:

Uses RandomForest to rank features by importance.

Keeps the top 15 features (you can adjust N).

Standardizes them (important for PCA).

Runs PCA, keeping 95% of variance.

Prints how much dimensionality was reduced.


Yes üëç ‚Äî for regression you can also use RandomForestRegressor to do feature filtering.

The logic is the same:

For classification ‚Üí RandomForestClassifier gives feature importance based on how well each feature splits the classes.

For regression ‚Üí RandomForestRegressor gives feature importance based on how much each feature reduces variance (MSE) across splits.


‚úÖ So yes ‚Äî RandomForest works for both classification and regression as a feature selection filter before PCA (or even directly as input to models).


NORMALIZATION = MinMaxScaler()

STANDARDIZATION - StandardScaler, RobustScaler

algorithms = {
        'Normalization (Min-Max)': [
            'K-Nearest Neighbors (KNN)',
            'Neural Networks / Deep Learning',
            'Support Vector Machines (linear kernels)',
            'K-Means Clustering',
            'Image Processing Models'
        ],
        'Standardization (Z-score)': [
            'Principal Component Analysis (PCA)',
            'Linear Regression',
            'Logistic Regression', 
            'Support Vector Machines (non-linear kernels)',
            'Linear Discriminant Analysis (LDA)',
            'Gaussian Naive Bayes'
        ],
        'No Scaling Needed': [
            'Decision Trees',
            'Random Forests',
            'Gradient Boosting Machines (XGBoost, LightGBM)',
            'Tree-based ensemble methods'
        ]
    }

questions = [
        "Does your algorithm assume normal distribution? ‚Üí STANDARDIZATION",
        "Do you need values in specific range (0-1)? ‚Üí NORMALIZATION", 
        "Does your data have significant outliers? ‚Üí ROBUST SCALING",
        "Are you using neural networks? ‚Üí NORMALIZATION",
        "Are you using distance-based algorithms? ‚Üí NORMALIZATION",
        "Are you using tree-based models? ‚Üí NO SCALING NEEDED",
        "Unsure? ‚Üí TRY STANDARDIZATION FIRST"
    ]

scenarios = {
        "Memorization vs Learning": "Model memorizes training data instead of learning patterns",
        "False Confidence": "95% accuracy on training data, but 60% on real world data", 
        "No Generalization Check": "No way to know if model works on new data",
        "Hyperparameter Guessing": "Tuning parameters without knowing real performance",
        "Deployment Risk": "High chance of failure in production"
    }


    datasets = {
        'Training Set (70-80%)': {
            'purpose': 'LEARN patterns and relationships',
            'analogy': 'Textbook and homework',
            'what_happens': 'Model parameters are updated',
            'risk': 'Overfitting (memorizing the textbook)'
        },
        'Validation Set (10-20%)': {
            'purpose': 'TUNE hyperparameters and model selection',
            'analogy': 'Practice exams', 
            'what_happens': 'Compare different models/parameters',
            'risk': 'Overfitting to validation set'
        },
        'Test Set (10-20%)': {
            'purpose': 'FINAL evaluation of chosen model',
            'analogy': 'Final exam',
            'what_happens': 'One-time assessment of real performance',
            'risk': 'None - this is the truth measurement'
        }

        benefits = {
        "Model Selection": "Compare different algorithms to choose the best one",
        "Hyperparameter Tuning": "Find optimal parameters without cheating",
        "Performance Estimation": "Get realistic estimate of real-world performance", 
        "Overfitting Detection": "Identify when model memorizes instead of learns",
        "Feature Selection": "Determine which features actually help prediction",
        "Early Stopping": "Stop training before overfitting occurs",
        "Confidence in Deployment": "Know your model will work in production"
    }

    """Show how splitting fits into the complete ML workflow"""
    
    workflow = [
        ("1. Data Collection", "Gather all available data"),
        ("2. Data Splitting", "Divide into Training/Validation/Test sets"),
        ("3. Model Training", "Learn patterns from Training set"), 
        ("4. Model Validation", "Tune parameters using Validation set"),
        ("5. Model Selection", "Choose best model based on Validation performance"),
        ("6. Final Evaluation", "Test chosen model on Test set (one time only)"),
        ("7. Deployment", "Use model in production on new, unseen data")
    ]


    pitfalls = [
        "OPTIMISM BIAS: Models always look better than they actually are",
        "DATA LEAKAGE: Information from 'future' data contaminates training",
        "HYPERPARAMETER OVERFITTING: Tuning parameters to noise in training data", 
        "UNREALISTIC EXPECTATIONS: Thinking your 95% training accuracy will work in real world",
        "WASTED RESOURCES: Deploying models that fail in production",
        "NO MODEL SELECTION: Can't tell which algorithm is actually best",
        "FALSE CONFIDENCE: Believing your model works when it doesn't"
    ]

Key Takeaways:

    üéØ Goal: Build models that work on NEW, unseen data

    üìä Training Set: For learning patterns (like textbook)

    üîç Validation Set: For tuning and model selection (like practice exams)

    üìù Test Set: For final evaluation (like final exam)

    üö® Without splitting: You can't know if your model actually works

    ‚úÖ With splitting: You get realistic performance estimates and avoid overfitting


reasons = [
        "Ensure your model learns general Jedi/Sith characteristics, not specific individuals",
        "Test if the model can classify knights it has never seen before", 
        "Compare different algorithms (KNN vs Random Forest vs Neural Network)",
        "Tune parameters like number of neighbors or tree depth properly",
        "Get realistic accuracy estimates for deployment",
        "Avoid building a model that only works on your specific sample"
    ]

Bottom line: Without splitting, you're essentially giving students the exam answers during study time, then being surprised when they fail the real exam! The validation set tells you if your model actually learned or just memorized.


steps = [
        ("1. Train Multiple Models", "Train the same algorithm with different hyperparameter values on the TRAINING set"),
        ("2. Validate Each Model", "Test each trained model on the VALIDATION set (unseen during training)"),
        ("3. Compare Performance", "See which hyperparameter combination gives best validation performance"),
        ("4. Select Best Model", "Choose the model configuration that performs best on validation data"),
        ("5. Final Test", "Test the selected model ONCE on the TEST set for final evaluation")
    ]

    purposes = {
        'Validation Set': {
            'Purpose': 'Model development and hyperparameter tuning',
            'Usage': 'Used repeatedly during model selection',
            'Analogy': 'Practice exams - try different strategies',
            'Risk': 'Can overfit to validation set with too much tuning'
        },
        'Test Set': {
            'Purpose': 'Final evaluation of chosen model',
            'Usage': 'Used ONCE after all decisions are made', 
            'Analogy': 'Final exam - one chance to prove yourself',
            'Risk': 'None - this is the unbiased truth measurement'
        }
    }

    Key Takeaways:

    üéØ Validation Purpose: Find optimal model configuration without cheating

    üîß What We Tune: Hyperparameters (k in KNN, tree depth in Random Forest, etc.)

    üìä How We Tune: Test different configurations on validation set

    üö® Never Tune on Test Set: That's data leakage and gives false confidence

    ‚úÖ Result: A model that genuinely works well on new, unseen data

    The validation set lets you answer:

    "What's the best number of neighbors for KNN to classify Jedi vs Sith?"

    "Should I use Random Forest or SVM for this problem?"

    "What parameters give the best generalization to new knights?"

        reference = {
        'X_train': 'Features for training the model',
        'y_train': 'Labels for training the model', 
        'X_val': 'Features for validating the model',
        'y_val': 'True labels for validation (to compare predictions)',
        'X_test': 'Features for final testing (if you have separate test set)',
        'y_test': 'True labels for final testing'
    }
    